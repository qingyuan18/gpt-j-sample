{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d245b130-4eaf-4fa1-b7f8-7eb5e1e9a7eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy gpt-j on SageMaker\n",
    "\n",
    "Now, we will deploy the model on SageMaker realtime endpoint, which is also trained on SageMaker with deepspeed on multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea489d40-d76e-4f19-a8b0-5755f7b1f23d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30d0c87-d3f7-4689-829e-d6b02fe9f60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572ed109-920f-4d6f-8215-e9f3e95e9718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_code/requirements.txt\n",
    "accelerate==0.16.0\n",
    "transformers==4.26.0\n",
    "bitsandbytes==0.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d366acc-b7cc-492b-952b-42db3d6ac75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_code/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=8\n",
    "option.s3url=s3://sagemaker-us-west-2-687912291502/llm/models/gpt-j/deepspeed/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c83fe-fb05-4f6a-a6ee-48aecee0cdb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## for local test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a23618-3c25-46db-8eeb-6a41e338ae95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./s5cmd sync s3://sagemaker-us-west-2-687912291502/llm/models/gpt-j/deepspeed/* ./model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3c7f22-aac1-4dbb-b3f3-afeffa00dc20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: accelerate==0.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from -r deploy_code/requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: transformers==4.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from -r deploy_code/requirements.txt (line 2)) (4.26.0)\n",
      "Requirement already satisfied: bitsandbytes==0.37.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from -r deploy_code/requirements.txt (line 3)) (0.37.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate==0.16.0->-r deploy_code/requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate==0.16.0->-r deploy_code/requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate==0.16.0->-r deploy_code/requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate==0.16.0->-r deploy_code/requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate==0.16.0->-r deploy_code/requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (4.63.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (0.13.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->accelerate==0.16.0->-r deploy_code/requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r deploy_code/requirements.txt (line 2)) (2022.12.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting deepspeed==0.8.0\n",
      "  Using cached https://publish.djl.ai/deepspeed/deepspeed-0.8.0-py2.py3-none-any.whl (756 kB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (5.9.4)\n",
      "Requirement already satisfied: ninja in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (1.11.1)\n",
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (1.10.4)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (9.0.0)\n",
      "Requirement already satisfied: hjson in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (3.1.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (1.23.5)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (4.63.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from deepspeed==0.8.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging->deepspeed==0.8.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pydantic->deepspeed==0.8.0) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r deploy_code/requirements.txt\n",
    "!pip install https://publish.djl.ai/deepspeed/deepspeed-0.8.0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fbc3e-9c7a-4fb0-8a87-c2cb4cfed82e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-25 06:43:33,438] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.8.0+4f7229c, git-hash=4f7229c, git-branch=HEAD\n",
      "[2023-04-25 06:43:33,442] [WARNING] [config_utils.py:67:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-04-25 06:43:33,443] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py39_cu117/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 1.8377962112426758 seconds\n",
      "[2023-04-25 06:43:37,539] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 16384, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'bf16': False, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 64, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': False, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tensor_parallel = 1\n",
    "model_location = \"/home/ec2-user/SageMaker/gpt-j-sample/deploy/djl_deepspeed/model/\"\n",
    "#model_location = \"EleutherAI/gpt-j-6b\"\n",
    "logging.info(f\"Loading model in {model_location}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_location,\n",
    "                                             low_cpu_mem_usage=True)\n",
    "\n",
    "data = [\"\"\"hello ,how are you ?\"\"\"]\n",
    "\n",
    "parameters = {\n",
    "  #\"early_stopping\": True,\n",
    "  #\"length_penalty\": 2.0,\n",
    "  \"max_new_tokens\": 50,\n",
    "  \"temperature\": 0,\n",
    "  \"min_length\": 10,\n",
    "  \"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "\n",
    "model.to(torch.float16)\n",
    "logging.info(f\"Starting DeepSpeed init with TP={tensor_parallel}\")\n",
    "model = deepspeed.init_inference(model,\n",
    "                                 mp_size=tensor_parallel,\n",
    "                                 dtype=model.dtype,\n",
    "                                 replace_method='auto',\n",
    "                                 replace_with_kernel_inject=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_tokens = tokenizer.batch_encode_plus(data,\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           padding=False)\n",
    "for t in input_tokens:\n",
    "    if torch.is_tensor(input_tokens[t]):\n",
    "        input_tokens[t] = input_tokens[t].to(torch.cuda.current_device())\n",
    "outputs = model.generate(**input_tokens, **parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b110bf-5212-4e07-9631-f4f3f92df506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello,how are you?\n",
      "\n",
      "I am fine,thanks for asking.\n",
      "How are things with you?\n",
      "Do you have any news?\n",
      "\n",
      "\n",
      "From: Tana Jones on 03/09/2001 10:30 AM\n",
      "To: Camille Gerard/Corp/\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f1761c-88ad-4e10-b705-09956238a038",
   "metadata": {
    "tags": []
   },
   "source": [
    "## djl deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e35c899-9c1a-443b-89b0-6f4ed27593c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location,\n",
    "                                                 low_cpu_mem_usage=True)\n",
    "    if \"dtype\" in properties:\n",
    "        if properties[\"dtype\"] == \"float16\":\n",
    "            model.to(torch.float16)\n",
    "        if properties[\"dtype\"] == \"bfloat16\":\n",
    "            model.to(torch.bfloat16)\n",
    "    \n",
    "    logging.info(f\"Starting DeepSpeed init with TP={tensor_parallel}\")\n",
    "    model = deepspeed.init_inference(model,\n",
    "                                     mp_size=tensor_parallel,\n",
    "                                     dtype=model.dtype,\n",
    "                                     replace_method='auto',\n",
    "                                     replace_with_kernel_inject=True)\n",
    "    return model.module, tokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, data, params):\n",
    "    generate_kwargs = params\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    input_tokens = tokenizer.batch_encode_plus(data,\n",
    "                                               return_tensors=\"pt\",\n",
    "                                               padding=True)\n",
    "    for t in input_tokens:\n",
    "        if torch.is_tensor(input_tokens[t]):\n",
    "            input_tokens[t] = input_tokens[t].to(torch.cuda.current_device())\n",
    "    outputs = model.generate(**input_tokens, **generate_kwargs)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    input_sentences = data[\"inputs\"]\n",
    "    params = data[\"parameters\"]\n",
    "    \n",
    "    outputs = run_inference(model, tokenizer, input_sentences, params)\n",
    "    result = {\"outputs\": outputs}\n",
    "    return Output().add_as_json(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3125b-fddf-458e-ad21-9fa61d923606",
   "metadata": {},
   "source": [
    "We will use LMI (large model inference) container on SageMaker to serve the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879f42da-c2dd-47a3-96ad-43dde83fdfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess._region_name\n",
    "\n",
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    ")\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a7070fe-3a46-4f28-976b-33da12a1880f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./requirements.txt\n",
      "./serving.properties\n",
      "./model.py\n"
     ]
    }
   ],
   "source": [
    "!rm model-gpt-j-deepspeed.tar.gz\n",
    "!tar czvf model-gpt-j-deepspeed.tar.gz -C deploy_code ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "265902c9-6406-45b7-b75f-2c1c5d86c65b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-687912291502/hf-large-model-djl-deploy/code-gpt-j-deepspeed/model-gpt-j-deepspeed.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = 'hf-large-model-djl-deploy/code-gpt-j-deepspeed'\n",
    "bucket = sess.default_bucket() \n",
    "s3_code_artifact = sess.upload_data(\"model-gpt-j-deepspeed.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0d7c064-964d-4c09-8592-2806e6325ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-gpt-j-deepspeed-2023-04-24-00-45-11-348\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\n",
      "Created Model: arn:aws:sagemaker:us-west-2:687912291502:model/code-gpt-j-deepspeed-2023-04-24-00-45-11-348\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "model_name = name_from_base(f\"code-gpt-j-deepspeed\")\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76f48151-f61d-4e27-8671-12108a9a882c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:687912291502:endpoint-config/code-gpt-j-deepspeed-2023-04-24-00-45-11-348-config-88',\n",
       " 'ResponseMetadata': {'RequestId': 'af3fcf19-4d00-46ea-98a6-5601eee20bb1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'af3fcf19-4d00-46ea-98a6-5601eee20bb1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '135',\n",
       "   'date': 'Mon, 24 Apr 2023 00:45:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config-88\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.48xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04c1882e-32c3-41c6-befd-ad9907d0b340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:687912291502:endpoint/code-gpt-j-deepspeed-2023-04-24-00-45-11-348-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c9520f0-c04b-4f59-8eab-9a5b287112a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:687912291502:endpoint/code-gpt-j-deepspeed-2023-04-24-00-45-11-348-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b9462-a2d8-4cda-a8cd-e068992f3dee",
   "metadata": {},
   "source": [
    "Use the low level boto3 API to generate context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3900294a-bea9-481d-a822-e72b69ce18fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 ms, sys: 0 ns, total: 15.5 ms\n",
      "Wall time: 1.13 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"outputs\":[\\n    \"hello,how are you??\\\\n\\\\I'm Fine\\\\how are you doing today?\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "prompts = \"\"\"Summarize the following news article:\n",
    "Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
    "Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well. Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompts = \"\"\"hello ,how are you ?\"\"\"\n",
    "\n",
    "parameters = {\n",
    "  #\"early_stopping\": True,\n",
    "  #\"length_penalty\": 2.0,\n",
    "  \"max_new_tokens\": 50,\n",
    "  \"temperature\": 0,\n",
    "  \"min_length\": 10,\n",
    "  \"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": [prompts],\n",
    "                \"parameters\": parameters\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "response_model['Body'].read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5acf45-1187-4cce-a57f-9cc499891f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
